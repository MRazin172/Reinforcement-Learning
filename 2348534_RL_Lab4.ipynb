{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwN4PCNhai1X6JI+UtWTgp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MRazin172/Reinforcement-Learning/blob/main/2348534_RL_Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets assume,\n",
        "\n",
        "it is an environment with a finite number of states and actions.\n",
        "\n",
        "and has defined rewards and transition probabilities."
      ],
      "metadata": {
        "id": "JSOWc4wl5S1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "WS3SW1o-6Rdk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MDP:\n",
        "    def __init__(self, states, actions, rewards, transition_probs, gamma=0.9):\n",
        "        self.states = states\n",
        "        self.actions = actions\n",
        "        self.rewards = rewards\n",
        "        self.transition_probs = transition_probs\n",
        "        self.gamma = gamma\n",
        "        self.values = np.zeros(len(states))\n",
        "        self.policy = np.random.choice(actions, len(states))\n",
        "\n",
        "    def policy_evaluation(self, threshold=1e-6):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for state in self.states:\n",
        "                v = self.values[state]\n",
        "                action = self.policy[state]\n",
        "                self.values[state] = sum(\n",
        "                    self.transition_probs.get((state, action, next_state), 0) *\n",
        "                    (self.rewards.get((state, action), 0) + self.gamma * self.values[next_state])\n",
        "                    for next_state in self.states\n",
        "                )\n",
        "                delta = max(delta, abs(v - self.values[state]))\n",
        "            if delta < threshold:\n",
        "                break\n",
        "\n",
        "    def policy_improvement(self):\n",
        "        policy_stable = True\n",
        "        for state in self.states:\n",
        "            old_action = self.policy[state]\n",
        "            action_values = []\n",
        "            for action in self.actions:\n",
        "                action_value = sum(\n",
        "                    self.transition_probs.get((state, action, next_state), 0) *\n",
        "                    (self.rewards.get((state, action), 0) + self.gamma * self.values[next_state])\n",
        "                    for next_state in self.states\n",
        "                )\n",
        "                action_values.append(action_value)\n",
        "            best_action = self.actions[np.argmax(action_values)]\n",
        "            self.policy[state] = best_action\n",
        "            if best_action != old_action:\n",
        "                policy_stable = False\n",
        "        return policy_stable\n",
        "\n",
        "    def policy_iteration(self):\n",
        "        iteration = 0\n",
        "        while True:\n",
        "            self.policy_evaluation()\n",
        "            policy_stable = self.policy_improvement()\n",
        "            iteration += 1\n",
        "            if policy_stable:\n",
        "                print(f\"Optimal policy found after {iteration} iterations.\")\n",
        "                break\n",
        "        return self.policy, self.values\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MbpBp7P05SiM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MDP class initializes the Markov Decision Process.\n",
        "\n",
        "It accepts states, actions, rewards, transition_probs, and gamma (discount factor) as inputs.\n",
        "\n",
        "self.values represents the estimated long-term values (returns) for each state, initialized to zero.\n",
        "\n",
        "self.policy is initialized randomly, choosing a random action for each state."
      ],
      "metadata": {
        "id": "tgA4a3UH5r2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initialization creates a setup for tracking the current policy and estimated values for each state. Random initialization encourages exploration from various actions.\n",
        "\n"
      ],
      "metadata": {
        "id": "y-A_ncfs5leG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method performs policy evaluation by iterating over each state and updating its value based on the current policy.\n",
        "\n",
        "For each state, it calculates the expected return (self.values[state]) as the sum of potential rewards and future values from following the current policy.\n",
        "\n",
        "It continues updating values until delta, the change in values from the previous iteration, is smaller than the threshold.\n",
        "\n",
        "Observation: This iterative approach (value iteration) calculates the expected long-term reward for each state-action pair, given the current policy. When delta is small, it indicates convergence, meaning state values are stable under the policy.\n",
        "\n"
      ],
      "metadata": {
        "id": "VynnqR2W5zso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method improves the policy by selecting the action with the highest expected return for each state, based on current state values.\n",
        "\n",
        "best_action is the action that maximizes the expected return, determined by evaluating all possible actions.\n",
        "\n",
        "If best_action differs from old_action, policy_stable is set to False, indicating further improvements are possible.\n",
        "\n",
        "Observation: This method attempts to optimize the policy by making greedy choices based on updated values. If no actions change, the policy is stable, signaling that the optimal policy has been found."
      ],
      "metadata": {
        "id": "NhQZn9_U5_I_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method combines policy evaluation and policy improvement iteratively.\n",
        "\n",
        "After each evaluation, it calls policy_improvement() to refine the policy.\n",
        "\n",
        "If policy_stable is True, it indicates that further policy improvement wonâ€™t yield better returns, so the algorithm stops.\n",
        "\n",
        "Observation: Policy iteration is an iterative approach that guarantees convergence to the optimal policy under certain conditions. It alternates between refining value estimates (policy evaluation) and optimizing actions (policy improvement).\n",
        "\n"
      ],
      "metadata": {
        "id": "9eVJfuLV6ECx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states = [0, 1, 2, 3]  # Four example states\n",
        "actions = [0, 1]       # Two possible actions\n",
        "rewards = {\n",
        "    (0, 0): 1, (0, 1): 0,\n",
        "    (1, 0): 0, (1, 1): 2,\n",
        "    (2, 0): 0, (2, 1): 3,\n",
        "    (3, 0): 1, (3, 1): 0\n",
        "}\n",
        "transition_probs = {\n",
        "    (0, 0, 0): 0.5, (0, 0, 1): 0.5,\n",
        "    (0, 1, 2): 1.0,\n",
        "    (1, 0, 2): 1.0,\n",
        "    (1, 1, 3): 1.0,\n",
        "    (2, 0, 3): 1.0,\n",
        "    (2, 1, 0): 1.0,\n",
        "    (3, 0, 0): 1.0,\n",
        "    (3, 1, 1): 1.0\n",
        "}"
      ],
      "metadata": {
        "id": "WGmBihwe6JaN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdp = MDP(states, actions, rewards, transition_probs)\n",
        "optimal_policy, optimal_values = mdp.policy_iteration()\n",
        "print(\"Optimal Policy:\", optimal_policy)\n",
        "print(\"Optimal State Values:\", optimal_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB5ZSkaR6yKn",
        "outputId": "cf7e796c-7f2a-4f74-a0b1-c014481b0794"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal policy found after 4 iterations.\n",
            "Optimal Policy: [1 1 1 0]\n",
            "Optimal State Values: [14.21052352 14.41052352 15.78947117 13.78947117]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Optimal Policy**: `[1 1 1 0]`\n",
        "   - This result suggests that, for states 0, 1, and 2, action 1 is optimal, while for state 3, action 0 is optimal.\n",
        "   - The actions chosen in the optimal policy maximize the expected cumulative rewards based on the given transition probabilities and rewards.\n",
        "   \n",
        "2. **Optimal State Values**: `[14.21, 14.41, 15.79, 13.79]`\n",
        "   - These values represent the expected cumulative reward for each state under the optimal policy.\n",
        "   - State 2 has the highest state value (15.79), which aligns with it being prioritized in action selection under the policy (since actions here should yield higher returns in the long run).\n",
        "   - The close values between states 0, 1, and 2 suggest the policy and values are converging and that state 3 has a slightly lower but stable value under its optimal action.\n",
        "\n",
        "3. **Number of Iterations**: `4`\n",
        "   - It took 4 iterations to reach a stable policy, which is reasonable for a small state space like this. Fewer than 10 iterations for policy convergence in small MDPs is typical.\n"
      ],
      "metadata": {
        "id": "XRNGs8Af7hbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary:\n",
        "This code demonstrates the **policy iteration** method to solve an MDP by alternating between policy evaluation and improvement. It efficiently converges to an optimal policy through iterative refinement, making it a robust solution for finite MDPs where optimal policies are desirable."
      ],
      "metadata": {
        "id": "0AgydEQY7puY"
      }
    }
  ]
}